{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'mini_train.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    mini_X = save['data']\n",
    "    mini_outcome = save['outcome']\n",
    "    del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=40\n",
    "num_unrollings=5\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, x_image, y_labels, batch_size, num_unrollings):\n",
    "        self._x_image = x_image\n",
    "        self._y_labels = y_labels\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._y_digits = self._extract_digits()\n",
    "        \n",
    "        \n",
    "    def _extract_digits(self):\n",
    "        end_digit = 10.0\n",
    "        \n",
    "        digits = np.ndarray(shape=(\n",
    "                self._num_unrollings, len(self._y_labels), int(end_digit + 1)), \n",
    "                            dtype=np.float32)\n",
    "        \n",
    "        for i in range(self._num_unrollings):\n",
    "            digit_coding = np.asarray( [x[i] if len(x)>i else end_digit \n",
    "                                        for x in self._y_labels])\n",
    "            digit_coding = (\n",
    "                np.arange(end_digit+1) == digit_coding[:,None]).astype(np.float32)\n",
    "            digits[i,:,:] = digit_coding\n",
    "        \n",
    "        return digits\n",
    "    \n",
    "    def next_batch(self):\n",
    "        idx = np.random.choice(self._x_image.shape[0],self._batch_size)\n",
    "        batch_x = self._x_image[idx,:,:,:]\n",
    "        batch_y = self._y_digits[:,idx,:]\n",
    "        \n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample a small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mini_train_batches = BatchGenerator(mini_X[:100], \n",
    "                                    mini_outcome['label'][:100],\n",
    "                                    batch_size, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 40, 11)\n",
      "(40, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = mini_train_batches.next_batch()\n",
    "print batch_y.shape\n",
    "print batch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# start a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = mini_X.shape[1]\n",
    "num_channels = mini_X.shape[3]\n",
    "CNN_num_nodes = 1024\n",
    "\n",
    "x_image = tf.placeholder(tf.float32, shape=(batch_size, \n",
    "                                            image_size, \n",
    "                                            image_size, num_channels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, num_channels, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([16 * 16 * 64, CNN_num_nodes])\n",
    "b_fc1 = bias_variable([CNN_num_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 16*16*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now connect with a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNN_num_nodes = 1024\n",
    "\n",
    "#11 collums for each digits, i.e., 0,1,...,9, and a ending ch <END>\n",
    "vocabulary_size = 11\n",
    "\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = weight_variable([vocabulary_size, RNN_num_nodes])\n",
    "im = weight_variable([RNN_num_nodes, RNN_num_nodes])\n",
    "ib = bias_variable([RNN_num_nodes])\n",
    "\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = weight_variable([vocabulary_size, RNN_num_nodes])\n",
    "fm = weight_variable([RNN_num_nodes, RNN_num_nodes])\n",
    "fb = bias_variable([RNN_num_nodes])\n",
    "\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = weight_variable([vocabulary_size, RNN_num_nodes])\n",
    "cm = weight_variable([RNN_num_nodes, RNN_num_nodes])\n",
    "cb = bias_variable([RNN_num_nodes])\n",
    "\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = weight_variable([vocabulary_size, RNN_num_nodes])\n",
    "om = weight_variable([RNN_num_nodes, RNN_num_nodes])\n",
    "ob = bias_variable([RNN_num_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the lstm cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "# state is cell state, o is hidden state, i is input\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# placeholder for digit input and digit labels\n",
    "digits_data = []\n",
    "for _ in range(num_unrollings + 1):\n",
    "    digits_data.append(\n",
    "        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    digits_inputs = digits_data[:num_unrollings]\n",
    "    digits_labels = digits_data[1:]  # labels are inputs shifted by one time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, RNN_num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, RNN_num_nodes]), trainable=False)\n",
    "\n",
    "#connect with CNN\n",
    "\n",
    "W_CNN = weight_variable([CNN_num_nodes, RNN_num_nodes])\n",
    "b_CNN = bias_variable([RNN_num_nodes])\n",
    "\n",
    "CNN_output = tf.matmul(h_fc1, W_CNN) + b_CNN\n",
    "\n",
    "output = saved_output + CNN_output\n",
    "state = saved_state + CNN_output\n",
    "\n",
    "# Unrolled LSTM loop.\n",
    "outputs = list()\n",
    "\n",
    "for i in digits_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier weights and biases.\n",
    "w_fc_rnn = weight_variable([RNN_num_nodes, vocabulary_size])\n",
    "b_fc_rnn = bias_variable([vocabulary_size])\n",
    "\n",
    "# State saving across unrollings.\n",
    "with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w_fc_rnn, b_fc_rnn)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, digits_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's check the prediction accuracy for 2st digit\n",
    "correct_prediction = tf.equal(tf.argmax(\n",
    "        tf.matmul(outputs[1], w_fc_rnn) + b_fc_rnn\n",
    "        ,1), \n",
    "                              tf.argmax(\n",
    "        digits_labels[1]\n",
    "        ,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on a small data to overfit\n",
    "if overfit, then ok. If not, check bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.217974\n",
      "step 0, training accuracy 0.2\n",
      "Average loss at step 20: 1.129356\n",
      "step 20, training accuracy 0.27625\n",
      "Average loss at step 40: 0.893669\n",
      "step 40, training accuracy 0.43875\n",
      "Average loss at step 60: 0.832818\n",
      "step 60, training accuracy 0.515\n",
      "Average loss at step 80: 0.731927\n",
      "step 80, training accuracy 0.6175\n",
      "Average loss at step 100: 0.686664\n",
      "step 100, training accuracy 0.65625\n",
      "Average loss at step 120: 0.604402\n",
      "step 120, training accuracy 0.72625\n",
      "Average loss at step 140: 0.543630\n",
      "step 140, training accuracy 0.79\n",
      "Average loss at step 160: 0.480441\n",
      "step 160, training accuracy 0.80125\n",
      "Average loss at step 180: 0.443382\n",
      "step 180, training accuracy 0.8675\n",
      "Average loss at step 200: 0.394744\n",
      "step 200, training accuracy 0.9175\n",
      "Average loss at step 220: 0.312721\n",
      "step 220, training accuracy 0.955\n",
      "Average loss at step 240: 0.264531\n",
      "step 240, training accuracy 0.96\n",
      "Average loss at step 260: 0.221890\n",
      "step 260, training accuracy 0.97125\n",
      "Average loss at step 280: 0.167557\n",
      "step 280, training accuracy 0.98625\n",
      "Average loss at step 300: 0.133320\n",
      "step 300, training accuracy 0.99375\n",
      "Average loss at step 320: 0.103768\n",
      "step 320, training accuracy 0.99875\n",
      "Average loss at step 340: 0.087972\n",
      "step 340, training accuracy 1\n",
      "Average loss at step 360: 0.071505\n",
      "step 360, training accuracy 1\n",
      "Average loss at step 380: 0.059441\n",
      "step 380, training accuracy 1\n",
      "Average loss at step 400: 0.050296\n",
      "step 400, training accuracy 1\n",
      "Average loss at step 420: 0.048009\n",
      "step 420, training accuracy 1\n",
      "Average loss at step 440: 0.039700\n",
      "step 440, training accuracy 1\n",
      "Average loss at step 460: 0.034613\n",
      "step 460, training accuracy 1\n",
      "Average loss at step 480: 0.029943\n",
      "step 480, training accuracy 1\n",
      "Average loss at step 500: 0.028714\n",
      "step 500, training accuracy 1\n",
      "Average loss at step 520: 0.024119\n",
      "step 520, training accuracy 1\n",
      "Average loss at step 540: 0.021070\n",
      "step 540, training accuracy 1\n",
      "Average loss at step 560: 0.020393\n",
      "step 560, training accuracy 1\n",
      "Average loss at step 580: 0.018360\n",
      "step 580, training accuracy 1\n",
      "Average loss at step 600: 0.016775\n",
      "step 600, training accuracy 1\n",
      "Average loss at step 620: 0.015905\n",
      "step 620, training accuracy 1\n",
      "Average loss at step 640: 0.014515\n",
      "step 640, training accuracy 1\n",
      "Average loss at step 660: 0.013302\n",
      "step 660, training accuracy 1\n",
      "Average loss at step 680: 0.013569\n",
      "step 680, training accuracy 1\n",
      "Average loss at step 700: 0.011875\n",
      "step 700, training accuracy 1\n",
      "Average loss at step 720: 0.011247\n",
      "step 720, training accuracy 1\n",
      "Average loss at step 740: 0.010443\n",
      "step 740, training accuracy 1\n",
      "Average loss at step 760: 0.010166\n",
      "step 760, training accuracy 1\n",
      "Average loss at step 780: 0.009609\n",
      "step 780, training accuracy 1\n",
      "Average loss at step 800: 0.008911\n",
      "step 800, training accuracy 1\n",
      "Average loss at step 820: 0.008663\n",
      "step 820, training accuracy 1\n",
      "Average loss at step 840: 0.008309\n",
      "step 840, training accuracy 1\n",
      "Average loss at step 860: 0.007734\n",
      "step 860, training accuracy 1\n",
      "Average loss at step 880: 0.007084\n",
      "step 880, training accuracy 1\n",
      "Average loss at step 900: 0.007402\n",
      "step 900, training accuracy 1\n",
      "Average loss at step 920: 0.006863\n",
      "step 920, training accuracy 1\n",
      "Average loss at step 940: 0.006531\n",
      "step 940, training accuracy 1\n",
      "Average loss at step 960: 0.006217\n",
      "step 960, training accuracy 1\n",
      "Average loss at step 980: 0.006617\n",
      "step 980, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "summary_frequency = 20\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss = 0\n",
    "mean_accuracy = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    batch_x, batch_y = mini_train_batches.next_batch()\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    feed_dict[x_image] = batch_x\n",
    "    \n",
    "    feed_dict[digits_data[0]] = np.zeros([batch_y.shape[1],batch_y.shape[2]])\n",
    "    \n",
    "    for i in range(num_unrollings):\n",
    "        feed_dict[digits_data[i+1]] = batch_y[i]\n",
    "            \n",
    "    _, l = sess.run(\n",
    "        [optimizer, loss], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    train_accuracy = accuracy.eval(feed_dict=feed_dict)\n",
    "    mean_accuracy += train_accuracy\n",
    "    \n",
    "    #now print something\n",
    "    if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "            mean_accuracy = mean_accuracy/ summary_frequency\n",
    "            \n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print('Average loss at step %d: %f' % (step, mean_loss))\n",
    "        mean_loss = 0\n",
    "        \n",
    "        \n",
    "        print(\"step %d, training accuracy %g\"%(step, mean_accuracy))\n",
    "        mean_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
